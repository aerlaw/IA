{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4111cab-a421-4873-9d05-5076a65d5d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 528 images belonging to 4 classes.\n",
      "Found 130 images belonging to 4 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 0.3958 - loss: -561.7024 - val_accuracy: 0.5000 - val_loss: -16021.0928\n",
      "Epoch 2/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.4634 - loss: -66812.6094 - val_accuracy: 0.5000 - val_loss: -709757.9375\n",
      "Epoch 3/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - accuracy: 0.4939 - loss: -1618476.8750 - val_accuracy: 0.5000 - val_loss: -9303899.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.4840 - loss: -16024226.0000 - val_accuracy: 0.5000 - val_loss: -63609296.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - accuracy: 0.4835 - loss: -93967144.0000 - val_accuracy: 0.5000 - val_loss: -291028800.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.4972 - loss: -393870944.0000 - val_accuracy: 0.5000 - val_loss: -1002832128.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - accuracy: 0.4745 - loss: -1266460416.0000 - val_accuracy: 0.5000 - val_loss: -2847825408.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.4800 - loss: -3484433664.0000 - val_accuracy: 0.5000 - val_loss: -7006800896.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.4925 - loss: -8111889920.0000 - val_accuracy: 0.5000 - val_loss: -15386096640.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.4779 - loss: -17037138944.0000 - val_accuracy: 0.5000 - val_loss: -30942900224.0000\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must provide at least one structure",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 88\u001b[0m\n\u001b[0;32m     79\u001b[0m test_generator \u001b[38;5;241m=\u001b[39m test_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[0;32m     80\u001b[0m     test_dir,\n\u001b[0;32m     81\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     85\u001b[0m )\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Faire des prédictions sur les images de test\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_generator)\n\u001b[0;32m     89\u001b[0m predicted_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(predictions \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mToxique\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComestible\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Afficher les résultats\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\tree\\optree_impl.py:76\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structures)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`func` must be callable. Received: func=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m structures:\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust provide at least one structure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m     78\u001b[0m     assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: Must provide at least one structure"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Définir les chemins des dossiers\n",
    "base_dir = r'C:\\Users\\Aerlaw\\Pictures\\IA'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir, 'champignon_test')\n",
    "\n",
    "# Créer des sous-dossiers pour les classes comestible et toxique dans le dossier train\n",
    "train_comestible_dir = os.path.join(base_dir, 'train_comestible')\n",
    "train_toxique_dir = os.path.join(base_dir, 'train_toxique')\n",
    "\n",
    "os.makedirs(train_comestible_dir, exist_ok=True)\n",
    "os.makedirs(train_toxique_dir, exist_ok=True)\n",
    "\n",
    "# Distribuer les images dans les sous-dossiers respectifs\n",
    "for filename in os.listdir(train_dir):\n",
    "    if filename.startswith('comestible'):\n",
    "        shutil.copy(os.path.join(train_dir, filename), os.path.join(train_comestible_dir, filename))\n",
    "    elif filename.startswith('toxique'):\n",
    "        shutil.copy(os.path.join(train_dir, filename), os.path.join(train_toxique_dir, filename))\n",
    "\n",
    "# Utilisation d'ImageDataGenerator pour la préparation des données\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Définir le modèle CNN\n",
    "model = Sequential([\n",
    "    tf.keras.Input(shape=(128, 128, 3)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entraîner le modèle\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "model.save('mushroom_classifier.keras')\n",
    "\n",
    "# Charger le modèle\n",
    "model = tf.keras.models.load_model('mushroom_classifier.keras')\n",
    "\n",
    "# Préparation des données de test\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode=None,  # Pas besoin des étiquettes pour la prédiction\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Faire des prédictions sur les images de test\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_classes = np.where(predictions > 0.5, 'Toxique', 'Comestible')\n",
    "\n",
    "# Afficher les résultats\n",
    "for filename, pred_class in zip(test_generator.filenames, predicted_classes):\n",
    "    print(f'Image: {filename} - Prédiction: {pred_class}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23b8378-057c-4c68-8e2c-0658dffe6044",
   "metadata": {},
   "source": [
    "# Test n°2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c702331-c197-4735-b229-1256ccd3c07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 195 images belonging to 2 classes.\n",
      "Found 130 images belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 2s/step - accuracy: 0.5054 - loss: 0.7101 - val_accuracy: 0.6172 - val_loss: 0.6887\n",
      "Epoch 2/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.6875 - loss: 0.6144 - val_accuracy: 0.0000e+00 - val_loss: 1.2212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aerlaw\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.5900 - loss: 0.7318 - val_accuracy: 0.6172 - val_loss: 0.6664\n",
      "Epoch 4/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6250 - loss: 0.6546 - val_accuracy: 0.0000e+00 - val_loss: 0.8679\n",
      "Epoch 5/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.5845 - loss: 0.6743 - val_accuracy: 0.6172 - val_loss: 0.6657\n",
      "Epoch 6/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5625 - loss: 0.6774 - val_accuracy: 0.0000e+00 - val_loss: 0.9124\n",
      "Epoch 7/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.6197 - loss: 0.6487 - val_accuracy: 0.6250 - val_loss: 0.6752\n",
      "Epoch 8/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7188 - loss: 0.6339 - val_accuracy: 0.5000 - val_loss: 0.7218\n",
      "Epoch 9/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.5906 - loss: 0.6647 - val_accuracy: 0.6484 - val_loss: 0.6762\n",
      "Epoch 10/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5625 - loss: 0.6937 - val_accuracy: 0.5000 - val_loss: 0.7121\n",
      "Epoch 11/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.6499 - loss: 0.6433 - val_accuracy: 0.6250 - val_loss: 0.6520\n",
      "Epoch 12/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5000 - loss: 0.6626 - val_accuracy: 0.0000e+00 - val_loss: 0.9074\n",
      "Epoch 13/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - accuracy: 0.6076 - loss: 0.6291 - val_accuracy: 0.6562 - val_loss: 0.6529\n",
      "Epoch 14/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7500 - loss: 0.6563 - val_accuracy: 0.5000 - val_loss: 0.7336\n",
      "Epoch 15/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - accuracy: 0.6863 - loss: 0.6282 - val_accuracy: 0.6016 - val_loss: 0.6677\n",
      "Epoch 16/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6875 - loss: 0.6360 - val_accuracy: 0.5000 - val_loss: 0.6433\n",
      "Epoch 17/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.6661 - loss: 0.6088 - val_accuracy: 0.6562 - val_loss: 0.6538\n",
      "Epoch 18/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5938 - loss: 0.6549 - val_accuracy: 0.5000 - val_loss: 0.9182\n",
      "Epoch 19/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.6643 - loss: 0.6136 - val_accuracy: 0.6406 - val_loss: 0.6686\n",
      "Epoch 20/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6250 - loss: 0.6625 - val_accuracy: 0.5000 - val_loss: 0.6426\n",
      "Epoch 21/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.7469 - loss: 0.5934 - val_accuracy: 0.5391 - val_loss: 0.6996\n",
      "Epoch 22/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6562 - loss: 0.5977 - val_accuracy: 0.5000 - val_loss: 0.5342\n",
      "Epoch 23/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.6841 - loss: 0.6000 - val_accuracy: 0.6719 - val_loss: 0.6570\n",
      "Epoch 24/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5938 - loss: 0.6091 - val_accuracy: 0.5000 - val_loss: 0.9213\n",
      "Epoch 25/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.6301 - loss: 0.6418 - val_accuracy: 0.6406 - val_loss: 0.6668\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.6390 - loss: 0.6500\n",
      "Loss: 0.6668643951416016, Accuracy: 0.6384615302085876\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 539ms/step\n",
      "image (1).jpg: Comestible\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "image (2).jpg: Comestible\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "image (3).jpg: Comestible\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "image (4).jpg: Toxique\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "image (5).jpg: Toxique\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "image (6).jpg: Comestible\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "image (7).jpg: Toxique\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "image (8).jpg: Comestible\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Chemins des répertoires\n",
    "train_dir = r'C:\\Users\\Aerlaw\\Pictures\\IA\\train'\n",
    "validation_dir = r'C:\\Users\\Aerlaw\\Pictures\\IA\\validation'\n",
    "test_images_dir = r'C:\\Users\\Aerlaw\\Pictures\\IA\\champignon_test'\n",
    "\n",
    "# Paramètres\n",
    "img_width, img_height = 150, 150\n",
    "batch_size = 32\n",
    "epochs = 25\n",
    "\n",
    "# Vérification des répertoires\n",
    "if not os.path.isdir(train_dir):\n",
    "    raise ValueError(f\"Le répertoire d'entraînement {train_dir} n'existe pas.\")\n",
    "if not os.path.isdir(validation_dir):\n",
    "    raise ValueError(f\"Le répertoire de validation {validation_dir} n'existe pas.\")\n",
    "if not os.path.isdir(test_images_dir):\n",
    "    raise ValueError(f\"Le répertoire de test {test_images_dir} n'existe pas.\")\n",
    "\n",
    "# Préparation des générateurs de données\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    classes=['comestible', 'toxique']\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    classes=['comestible', 'toxique'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Construction du modèle CNN\n",
    "model = Sequential([\n",
    "    Input(shape=(img_width, img_height, 3)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entraînement du modèle\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Évaluation du modèle\n",
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
    "\n",
    "# Prédiction sur de nouvelles images\n",
    "def predict_image(image_path):\n",
    "    img = image.load_img(image_path, target_size=(img_width, img_height))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
    "    \n",
    "    prediction = model.predict(img_array)\n",
    "    \n",
    "    if prediction < 0.5:\n",
    "        return 'Comestible'\n",
    "    else:\n",
    "        return 'Toxique'\n",
    "\n",
    "# Exemple d'utilisation\n",
    "for file in os.listdir(test_images_dir):\n",
    "    img_path = os.path.join(test_images_dir, file)\n",
    "    if os.path.isfile(img_path):\n",
    "        prediction = predict_image(img_path)\n",
    "        print(f'{file}: {prediction}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6c9cb6b-b209-45f6-a57f-ecbdd77d12cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 195 images belonging to 2 classes.\n",
      "Found 130 images belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 4s/step - accuracy: 0.5074 - loss: 0.7200 - val_accuracy: 0.3906 - val_loss: 0.7044\n",
      "Epoch 2/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.4630 - loss: 0.7014 - val_accuracy: 0.8061 - val_loss: 0.5081\n",
      "Epoch 3/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - accuracy: 0.6128 - loss: 0.6697 - val_accuracy: 0.6531 - val_loss: 0.6475\n",
      "Epoch 4/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.5758 - loss: 0.6704 - val_accuracy: 0.5408 - val_loss: 0.6903\n",
      "Epoch 5/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.6299 - loss: 0.6645 - val_accuracy: 0.4796 - val_loss: 0.7119\n",
      "Epoch 6/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.5926 - loss: 0.6644 - val_accuracy: 0.6172 - val_loss: 0.6663\n",
      "Epoch 7/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.5810 - loss: 0.6756 - val_accuracy: 0.8061 - val_loss: 0.5505\n",
      "Epoch 8/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.5989 - loss: 0.6591 - val_accuracy: 0.6633 - val_loss: 0.6439\n",
      "Epoch 9/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.5981 - loss: 0.6619 - val_accuracy: 0.5714 - val_loss: 0.6901\n",
      "Epoch 10/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.6296 - loss: 0.6471 - val_accuracy: 0.5816 - val_loss: 0.6757\n",
      "Epoch 11/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.6420 - loss: 0.6624 - val_accuracy: 0.6328 - val_loss: 0.6529\n",
      "Epoch 12/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.6205 - loss: 0.6144 - val_accuracy: 0.8061 - val_loss: 0.5297\n",
      "Epoch 13/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.5611 - loss: 0.6641 - val_accuracy: 0.6939 - val_loss: 0.6288\n",
      "Epoch 14/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.6181 - loss: 0.6293 - val_accuracy: 0.5816 - val_loss: 0.6803\n",
      "Epoch 15/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.6829 - loss: 0.6301 - val_accuracy: 0.5918 - val_loss: 0.6981\n",
      "Epoch 16/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.7163 - loss: 0.5716 - val_accuracy: 0.6484 - val_loss: 0.6521\n",
      "Epoch 17/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.5950 - loss: 0.6352 - val_accuracy: 0.6122 - val_loss: 0.6459\n",
      "Epoch 18/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7450 - loss: 0.5862 - val_accuracy: 0.5102 - val_loss: 0.6986\n",
      "Epoch 19/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.6955 - loss: 0.6060 - val_accuracy: 0.5918 - val_loss: 0.7173\n",
      "Epoch 20/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.6229 - loss: 0.6080 - val_accuracy: 0.6020 - val_loss: 0.6877\n",
      "Epoch 21/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.7396 - loss: 0.5557 - val_accuracy: 0.5781 - val_loss: 0.6679\n",
      "Epoch 22/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.7051 - loss: 0.5867 - val_accuracy: 0.6531 - val_loss: 0.6230\n",
      "Epoch 23/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.6744 - loss: 0.5588 - val_accuracy: 0.7245 - val_loss: 0.6101\n",
      "Epoch 24/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.6671 - loss: 0.5674 - val_accuracy: 0.5918 - val_loss: 0.6899\n",
      "Epoch 25/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7459 - loss: 0.5385 - val_accuracy: 0.6327 - val_loss: 0.6679\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - accuracy: 0.6770 - loss: 0.6311\n",
      "Loss: 0.6678784489631653, Accuracy: 0.6326530575752258\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step\n",
      "image (1).jpg: Comestible\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "image (2).jpg: Comestible\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "image (3).jpg: Comestible\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "image (4).jpg: Toxique\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "image (5).jpg: Toxique\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "image (6).jpg: Toxique\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "image (7).jpg: Toxique\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "image (8).jpg: Comestible\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Chemins des répertoires\n",
    "train_dir = r'C:\\Users\\Aerlaw\\Pictures\\IA\\train'\n",
    "validation_dir = r'C:\\Users\\Aerlaw\\Pictures\\IA\\validation'\n",
    "test_images_dir = r'C:\\Users\\Aerlaw\\Pictures\\IA\\champignon_test'\n",
    "\n",
    "# Paramètres\n",
    "img_width, img_height = 150, 150\n",
    "batch_size = 32\n",
    "epochs = 25\n",
    "\n",
    "# Vérification des répertoires\n",
    "if not os.path.isdir(train_dir):\n",
    "    raise ValueError(f\"Le répertoire d'entraînement {train_dir} n'existe pas.\")\n",
    "if not os.path.isdir(validation_dir):\n",
    "    raise ValueError(f\"Le répertoire de validation {validation_dir} n'existe pas.\")\n",
    "if not os.path.isdir(test_images_dir):\n",
    "    raise ValueError(f\"Le répertoire de test {test_images_dir} n'existe pas.\")\n",
    "\n",
    "# Préparation des générateurs de données\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    classes=['comestible', 'toxique']\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    classes=['comestible', 'toxique'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Répétition des générateurs\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: train_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, img_width, img_height, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float32))\n",
    ").repeat()\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: validation_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, img_width, img_height, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float32))\n",
    ").repeat()\n",
    "\n",
    "# Construction du modèle CNN\n",
    "model = Sequential([\n",
    "    Input(shape=(img_width, img_height, 3)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entraînement du modèle\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_dataset,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Évaluation du modèle\n",
    "loss, accuracy = model.evaluate(validation_dataset, steps=validation_generator.samples // batch_size)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
    "\n",
    "# Prédiction sur de nouvelles images\n",
    "def predict_image(image_path):\n",
    "    img = image.load_img(image_path, target_size=(img_width, img_height))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
    "    \n",
    "    prediction = model.predict(img_array)\n",
    "    \n",
    "    if prediction < 0.5:\n",
    "        return 'Comestible'\n",
    "    else:\n",
    "        return 'Toxique'\n",
    "\n",
    "# Exemple d'utilisation\n",
    "for file in os.listdir(test_images_dir):\n",
    "    img_path = os.path.join(test_images_dir, file)\n",
    "    if os.path.isfile(img_path):\n",
    "        prediction = predict_image(img_path)\n",
    "        print(f'{file}: {prediction}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c00255-0a78-4a04-b17a-28ae5d9c411d",
   "metadata": {},
   "source": [
    "# Nouveau test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fad50c36-a0f5-4b10-be06-f3d5db794f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 195 images belonging to 2 classes.\n",
      "Found 130 images belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 4s/step - accuracy: 0.5211 - loss: 0.7153 - val_accuracy: 0.6172 - val_loss: 0.6977\n",
      "Epoch 2/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.5914 - loss: 0.6988 - val_accuracy: 0.8061 - val_loss: 0.5831\n",
      "Epoch 3/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2s/step - accuracy: 0.5585 - loss: 0.6769 - val_accuracy: 0.6531 - val_loss: 0.6351\n",
      "Epoch 4/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.5369 - loss: 0.7067 - val_accuracy: 0.4796 - val_loss: 0.7235\n",
      "Epoch 5/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.6676 - loss: 0.6025 - val_accuracy: 0.4898 - val_loss: 0.7287\n",
      "Epoch 6/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.5895 - loss: 0.6521 - val_accuracy: 0.6641 - val_loss: 0.6597\n",
      "Epoch 7/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 3s/step - accuracy: 0.6213 - loss: 0.6455 - val_accuracy: 0.7959 - val_loss: 0.5923\n",
      "Epoch 8/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 0.6155 - loss: 0.6472 - val_accuracy: 0.7245 - val_loss: 0.6336\n",
      "Epoch 9/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3s/step - accuracy: 0.7334 - loss: 0.5931 - val_accuracy: 0.5102 - val_loss: 0.7411\n",
      "Epoch 10/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.6110 - loss: 0.6444 - val_accuracy: 0.5816 - val_loss: 0.6640\n",
      "Epoch 11/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.6509 - loss: 0.6308 - val_accuracy: 0.6641 - val_loss: 0.6435\n",
      "Epoch 12/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.6758 - loss: 0.5900 - val_accuracy: 0.5000 - val_loss: 0.7282\n",
      "Epoch 13/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.6413 - loss: 0.6128 - val_accuracy: 0.5918 - val_loss: 0.6303\n",
      "Epoch 14/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.6582 - loss: 0.6038 - val_accuracy: 0.5816 - val_loss: 0.6860\n",
      "Epoch 15/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.6985 - loss: 0.6062 - val_accuracy: 0.6327 - val_loss: 0.6511\n",
      "Epoch 16/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7029 - loss: 0.5759 - val_accuracy: 0.6562 - val_loss: 0.6383\n",
      "Epoch 17/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7898 - loss: 0.4863 - val_accuracy: 0.6122 - val_loss: 0.6418\n",
      "Epoch 18/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.6880 - loss: 0.5544 - val_accuracy: 0.5612 - val_loss: 0.7040\n",
      "Epoch 19/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7183 - loss: 0.5498 - val_accuracy: 0.5918 - val_loss: 0.7053\n",
      "Epoch 20/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.6915 - loss: 0.5625 - val_accuracy: 0.6429 - val_loss: 0.6760\n",
      "Epoch 21/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.7288 - loss: 0.4953 - val_accuracy: 0.6406 - val_loss: 0.6617\n",
      "Epoch 22/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.6997 - loss: 0.5430 - val_accuracy: 0.7347 - val_loss: 0.5819\n",
      "Epoch 23/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3s/step - accuracy: 0.6783 - loss: 0.6190 - val_accuracy: 0.5816 - val_loss: 0.6751\n",
      "Epoch 24/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7374 - loss: 0.5048 - val_accuracy: 0.5816 - val_loss: 0.7430\n",
      "Epoch 25/25\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.7040 - loss: 0.5532 - val_accuracy: 0.6020 - val_loss: 0.6992\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 904ms/step - accuracy: 0.6898 - loss: 0.6257\n",
      "Loss: 0.6992199420928955, Accuracy: 0.6020408272743225\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379ms/step\n",
      "champi_C1.jpg: Comestible\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "champi_C2.jpg: Comestible\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "champi_C3.jpg: Comestible\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "champi_C4.jpg: Comestible\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "champi_C5jpg.jpg: Toxique\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "champi_T1.jpg: Comestible\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "champi_T2.jpg: Toxique\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "champi_T3.jpg: Comestible\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "champi_T4.jpg: Comestible\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "champi_T5.jpg: Comestible\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Chemins des répertoires\n",
    "train_dir = r'C:\\Users\\Aerlaw\\Pictures\\IA\\train'\n",
    "validation_dir = r'C:\\Users\\Aerlaw\\Pictures\\IA\\validation'\n",
    "test_images_dir = r'C:\\Users\\Aerlaw\\Pictures\\IA\\champignon_test'\n",
    "\n",
    "# Paramètres\n",
    "img_width, img_height = 150, 150\n",
    "batch_size = 32\n",
    "epochs = 25\n",
    "\n",
    "# Vérification des répertoires\n",
    "if not os.path.isdir(train_dir):\n",
    "    raise ValueError(f\"Le répertoire d'entraînement {train_dir} n'existe pas.\")\n",
    "if not os.path.isdir(validation_dir):\n",
    "    raise ValueError(f\"Le répertoire de validation {validation_dir} n'existe pas.\")\n",
    "if not os.path.isdir(test_images_dir):\n",
    "    raise ValueError(f\"Le répertoire de test {test_images_dir} n'existe pas.\")\n",
    "\n",
    "# Préparation des générateurs de données\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    classes=['comestible', 'toxique']\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    classes=['comestible', 'toxique'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Répétition des générateurs\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: train_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, img_width, img_height, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float32))\n",
    ").repeat()\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: validation_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, img_width, img_height, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float32))\n",
    ").repeat()\n",
    "\n",
    "# Construction du modèle CNN\n",
    "model = Sequential([\n",
    "    Input(shape=(img_width, img_height, 3)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entraînement du modèle\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_dataset,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Évaluation du modèle\n",
    "loss, accuracy = model.evaluate(validation_dataset, steps=validation_generator.samples // batch_size)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
    "\n",
    "# Prédiction sur de nouvelles images\n",
    "def predict_image(image_path):\n",
    "    img = image.load_img(image_path, target_size=(img_width, img_height))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
    "    \n",
    "    prediction = model.predict(img_array)\n",
    "    \n",
    "    if prediction < 0.5:\n",
    "        return 'Comestible'\n",
    "    else:\n",
    "        return 'Toxique'\n",
    "\n",
    "# Exemple d'utilisation\n",
    "for file in os.listdir(test_images_dir):\n",
    "    img_path = os.path.join(test_images_dir, file)\n",
    "    if os.path.isfile(img_path):\n",
    "        prediction = predict_image(img_path)\n",
    "        print(f'{file}: {prediction}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34049950-38ee-4e93-b70c-884789b7276b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
